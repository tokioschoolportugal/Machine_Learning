{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: Dataset de rostos de Olivetti\n",
    "\n",
    "## O que vamos fazer?\n",
    "- Reduzir dimensionalidade de um dataset de alta dimensionalidade como LFW \n",
    "- Escolher um subconjunto mínimo de vetores\n",
    "- Representar os principais vetores\n",
    "\n",
    "No exercício anterior vimos como implementar o PCA com Scikit-learn para reduzir a dimensionalidade, particularmente para a representação dos dados.\n",
    "\n",
    "Neste exercício, vamos centrar-nos em encontrar o número mínimo de componentes principais para os quais podemos reduzir a dimensionalidade do dataset perdendo o mínimo de informação ou variação possível.\n",
    "\n",
    "Para isso, vamos utilizar o dataset (rostos de Olivetti)[https://scikit-learn.org/stable/datasets/real_world.html#olivetti-faces-dataset] para explorar a representação gráfica dos principais componentes na classificação de imagens e a sua relação com as imagens originais.\n",
    "\n",
    "\n",
    "*NOTA*:  Para este exercício pode basear-se nas seguintes páginas da documentação Scikit-learn:\n",
    "\n",
    "- [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "- [Faces dataset decompositions](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html).\n",
    "\n",
    "*PD: Normalmente, o trabalho de um engenheiro de software é saber como procurar na documentação e exemplos na Internet como fazer algo, e modificar esse código para o seu caso de uso. Portanto, neste e noutros exercícios convidamo-lo a trabalhar na composição de código a partir de partes de outros exemplos =).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Importar todas as livrarias necessárias aqui\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "fignum = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O dataset de rostos de Olivetti\n",
    "\n",
    "Primeiro vamos descarregar o dataset de rostos Olivetti, extrair os primeiros 6 e representá-los num gráfico.\n",
    "\n",
    "Para isso, seguir as instruções para completar a seguinte célula de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração da representação das imagens\n",
    "\n",
    "n_col = 3   # N.º de colunas e filas do gráfico\n",
    "n_row = 2   # Gráfico de 2x3 imagens\n",
    "image_shape = (64, 64) # Tamanho em px da imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Descarregar o dataset de rostos Olivetti, extrair os primeiros 6 rostos e representá-los num gráfico.\n",
    "\n",
    "# Descarregar as imagens do dataset\n",
    "# Para este exemplo não vamos a classificar o dataset, pelo que podemos descartar as classes\n",
    "faces, _ = datasets.fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=rng)\n",
    "\n",
    "# Extrair o n.º de exemplos e características do mesmo, m e n\n",
    "m, n = [...]\n",
    "\n",
    "# Extrair as 6 primeiras imagens\n",
    "faces = [...]\n",
    "\n",
    "# Centrar as imagens\n",
    "# Para isso, pode basear-se no exemplo mencionado previamente\n",
    "faces_centered = [...]\n",
    "\n",
    "# Representar graficamente as 6 primeiras imagens\n",
    "plt.figure(fignum, figsize=(2. * n_col, 2.25 * n_row)) plt.suptitle('6 primeiras imagens originais')\n",
    "\n",
    "for i, face in \n",
    "    enumerate(faces_centered): \n",
    "    plt.subplot(n_row, n_col, i + 1)\n",
    "    vmax = max(face.max(), -face.min())\n",
    "    \n",
    "    plt.imshow(face.reshape(image_reshape), cmap=plt.cm.gray, interpolation='nearest', vmin=-vmax, vmax=vmax) plt.xticks(())\n",
    "    \n",
    "    plt.yticks(())\n",
    "    \n",
    "plt.subplots_adjust(0,01, 0,05, 0,99, 0,93, 0,04, 0.)\n",
    "\n",
    "fignum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redução da dimensionalidade\n",
    "\n",
    "Agora vamos comprovar como é a representação gráfica de uma redução de dimensionalidade em imagens.\n",
    "\n",
    "Ao passar das imagens originais aos componentes principais, veremos como o PCA mostra as principais características de cada imagem. \n",
    "\n",
    "Seguir as instruções na célula seguinte para representar os 6 primeiros componentes principais por PCA desses rostos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar os 6 primeiros componentes principais dos rostos e representá-los graficamente\n",
    "\n",
    "pca_6 = PCA(n_components=6, svd_solver='randomized', whiten=True) components = pca_6.fit([...]).components_\n",
    "\n",
    "# Representar os componentes principais para os 6 primeiros rostos\n",
    "plt.figure(fignum, figsize=(2. * n_col, 2.25 * n_row)) [...]\n",
    "\n",
    "fignum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolher o n.º ótimo de componentes principais\n",
    "\n",
    "No exemplo acima, obtivemos os primeiros 6 componentes principais. No entanto, este número de componentes tem sido um número escolhido à vontade.\n",
    "\n",
    "Normalmente, para reduzir a dimensionalidade de um dataset quando se forma um modelo ML, queremos reduzi-lo tanto quanto possível, encontrando o número mínimo de componentes que mantenha o máximo de informação ou variação do dataset original pelo método [MLE de Minka](https://vismod.media.mit.edu/tech-reports/TR-514.pdf).\n",
    "\n",
    "Para isso, antes de continuar, fazer as seguintes perguntas:\n",
    "- *O que representa o parâmetro de \"sdv_solver\" do método PCA()?*\n",
    "- *E o de “n_components”, e a sua relação com \"sdv_solver\"?*\n",
    "- *Que método implementa o “n_components == 'mle'\"?*\n",
    "- *O que representam os atributos do método PCA()? \"components_\", \"explainedvariance\", etc.*\n",
    "- *Em que ordem se ordenam os \"components_\" que devolve o PCA()?*\n",
    "\n",
    "Para encontrar o número mínimo de componentes principais, seguir as instruções para completar a seguinte célula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escolher o n.º ótimo de componentes principais\n",
    "\n",
    "pca_min = PCA(n_components='mle') \n",
    "\n",
    "components = pca_min.fit([...]).components_\n",
    "\n",
    "# Representar os componentes principais para os 6 primeiros rostos\n",
    "plt.figure(fignum, figsize=(2. * n_col, 2.25 * n_row)) [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método MLE de Minka tenta estabelecer uma heurística para escolher do número de componentes.\n",
    "\n",
    "Podemos também escolher uma percentagem mínima de variância ou informação que queremos preservar o dataset, e o método \n",
    "\n",
    "PCA irá devolver o n.º mínimo de componentes que preservam essa percentagem ou mais.\n",
    "\n",
    "Para isso, seguir as instruções na célula seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escolher o n.º ótimo de componentes principais\n",
    "\n",
    "min_var = 0.9    # Variância mín. a preservar\n",
    "\n",
    "pca_min_var = PCA(n_components=min_var, svd_solver='full')\n",
    "\n",
    "pca_min_var_fitted = pca_min_var.fit([...])\n",
    "\n",
    "# Representar os componentes principais para os 6 primeiros rostos\n",
    "plt.figure(fignum, figsize=(2. * n_col, 2.25 * n_row)) [...]\n",
    "\n",
    "fignum += 1\n",
    "\n",
    "# Analisar o n.º de componentes e a variância explicada por cada um deles\n",
    "\n",
    "print('N.º de componentes:',len(pca_min_var_fitted.components_)) \n",
    "print('Ratio de variância explicada:') \n",
    "print(pca_min_var_fitted.explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variar o valor da variância mínima a preservar e anotar o número de componentes e a variância explicada por cada um deles.\n",
    "\n",
    "*Quantos componentes são preservados com uma variância mínima de 100%? E 95%, 90%, 85%, 75% e 50%?*\n",
    "\n",
    "*Como é que os ratios de variância explicados nos primeiros componentes devolvidos em cada caso?*"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
