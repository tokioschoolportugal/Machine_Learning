{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão logística: Função de custo e formação\n",
    "\n",
    "## O que vamos fazer?\n",
    "- Criar um dataset sintético para regressão logística manualmente e com Scikit-learn. \n",
    "- Implementar a função de ativação logística sigmoide.\n",
    "- Implementar a função de custo regularizado para a regressão logística. \n",
    "- Implementar a formação do modelo por gradient descent.\n",
    "- Comprovar a formação representando a evolução da função custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de um dataset sintético para regressão logística\n",
    "\n",
    "Vamos criar novamente um dataset sintéticos, mas desta vez para regressão logística.\n",
    "\n",
    "Vamos descobrir como fazê-lo com os 2 métodos que utilizámos anteriormente: manualmente e com Scikit-learn, usando a função  [sklearn_datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Gerar um dataset sintético, com o termo de bias e erro de forma manual\n",
    "m = 100\n",
    "n = 2\n",
    "\n",
    "# Gerar um array 2D m x n com valores números aleatórios entre -1 e 1.\n",
    "# Inserir o termo de bias como primeira coluna de 1s\n",
    "\n",
    "X = [...]\n",
    "\n",
    "# Gerar um array de theta de n + 1 valores aleatórios\n",
    "Theta_verd = [...]\n",
    "\n",
    "# Calcular Y em função de X e Theta_verd\n",
    "# Adicionar um termo de erro modificável\n",
    "# Transformar Y para valores de 1. e 0. (float) quando Y >= 0,0\n",
    "error = 0.15\n",
    "\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "\n",
    "# Comprovar os valores e dimensões dos vetores\n",
    "print('Theta a estimar') \n",
    "print()\n",
    "\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:') \n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensões de X e Y:') \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Gerar um dataset sintéticos, com o termo de bias e erro com Scikit-learn\n",
    "\n",
    "# Utilizar os mesmos valores de m, n e erro do dataset anterior\n",
    "X_sklearn = [...]\n",
    "Y_sklearn = [...]\n",
    "\n",
    "# Comprovar os valores e dimensões dos vetores\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:') \n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensões de X e Y:') \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que com o método Scikit-learn não podemos recuperar os coeficientes utilizados, vamos usar o método manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a função sigmoide\n",
    "\n",
    "Vamos implementar a função de ativação sigmoide. Vamos usar esta função para implementar a nossa hipótese, que transforma as previsões do modelo em valores de 0 e 1.\n",
    "\n",
    "Função sigmoide:\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "Y = h_\\theta(x) = g(\\Theta \\times X) = \\frac{1}{1 + e^{-\\Theta^Tx}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função de ativação sigmóide.\n",
    "\n",
    "def sigmoid(theta, x):\n",
    "    \"\"\" Devolver o valor do sigmoide para essa theta y x\n",
    "        Argumentos posicionais:\n",
    "        theta -- array 1D de Numpy com a fila ou coluna de coeficientes das características \n",
    "        x -- array 1D de Numpy com as características de um exemplo\n",
    "        Devolver:\n",
    "        sigmoide -- float com o valor do sigmoide para esses parâmetros\n",
    "    \"\"\"\n",
    "    \n",
    "    return [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a função de custo regularizada\n",
    "\n",
    "Vamos implementar a função de custo regularizada. Esta função será semelhante à que implementámos para a regressão linear num exercício anterior\n",
    "\n",
    "Função de custo regularizada\n",
    "\n",
    "$J(\\Theta) = - [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (y^i log(h_\\theta(x^i)) + (1 - y^i) log(1 - h_\\theta(x^i))] \\\\\n",
    "+ \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\Theta_j^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função de custo regularizado para a regressão logística\n",
    "\n",
    "def regularized_logistic_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computar a função de custo para o dataset e coeficientes considerados\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    i -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, 1 e valores 0 ou 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila) \n",
    "    lambda_ -- fator de regularização, por defeito 0.\n",
    "    \n",
    "    Devolver:\n",
    "    j -- float com o custo para esse array theta \n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Recordar de comprovar as dimensões da multiplicação da matriz para a fazer corretamente.\n",
    "    j = [...]\n",
    "    \n",
    "    # Regularizar para todos os Theta exceto o termo bias (o primeiro valor)\n",
    "    j += [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como em exercícios anteriores, comprovar a sua implementação calculando a função de custo para cada exemplo do dataset.\n",
    "\n",
    "Com o Y correto e a *lambda* a 0, a função de custo também deve ser 0. À medida que o *theta* se afasta ou a *lambda* aumenta, o custo deve ser superior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprovar a sua implementação no dataset\n",
    "\n",
    "theta = Theta_verd # Modificar e comprovar vários valores de theta \n",
    "\n",
    "j = regularized_logistic_cost_function(X, Y, theta, lambda_=0.) \n",
    "\n",
    "print('Custo do modelo:')\n",
    "print(j)\n",
    "print('Theta comprovado e Theta real:') \n",
    "print(theta)\n",
    "print(Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a formação por gradient descent\n",
    "\n",
    "Vamos agora otimizar esta função de custos, para formar o nosso modelo através de gradient descent \n",
    "regularizado. \n",
    "\n",
    "No exercício seguinte vamos usar a regularização para efetuar a validação cruzada.\n",
    "\n",
    "Atualizações dos coeficientes *theta*:\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=0}^{m} (h_\\theta (x^i) - y^i) x_0^i \\\\\n",
    "\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum\\limits_{i=0}^{m} (h_\\theta (x^i) - y^i) x_0^i + \\frac{\\lambda}{m} \\theta_j]; \\\\\n",
    "j \\in [1, n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função que forma o modelo por gradient descent regularizado\n",
    "\n",
    "def regularized_logistic_gradient_descent(x, y, theta, alpha=1e-1, lambda_=0., e=1e-3, iter_=1e3): \"\"\" \n",
    "    Formar o modelo otimizando a sua função de custo por gradient descent\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    x -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, de tamanho m x 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila)\n",
    "    \n",
    "    Argumentos numerados (keyword):\n",
    "    alpha -- float, ratio de formação\n",
    "    lambda -- float com o parâmetro de regularização\n",
    "    e -- float, diferença mínima entre iterações para declarar que a formação finalmente convergiu \n",
    "    iter_ -- int/float, número de iterações\n",
    "    \n",
    "    Devolver:\n",
    "    j_hist -- list/array com a evolução da função de custo durante a formação \n",
    "    theta -- array Numpy com o valor do theta na última iteração\n",
    "    \"\"\"\n",
    "    iter_ = int(iter_) # Se declarou iter_ em notação científica (1e3) ou float (1000.), converta-o\n",
    "    \n",
    "    # Inicializar j_hist como uma list ou um array Numpy. Recordar que não sabemos que tamanho terá eventualmente\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...] # Obter m e n a partir das dimensões de X\n",
    "    \n",
    "    for k in [...]: # Iterar sobre o número máximo de iterações\n",
    "        theta_iter = [...] # Declarar um theta para cada iteração, pois precisamos de a atualizar.\n",
    "        \n",
    "        for j in [...]: # Iterar sobre o número de características\n",
    "            # Atualizar theta_iter para cada característica, de acordo com a derivada da função de custo\n",
    "            # Incluir a relação de formação alfa\n",
    "            # Cuidado com as multiplicações de matriz, a sua ordem e dimensões\n",
    "            \n",
    "            if j > 0:\n",
    "                pass # Regularizar tudo coeficiente exceto o do parâmetro bias (primeiro valor)\n",
    "            \n",
    "            theta_iter[j] = theta[j] - [...] \n",
    "            \n",
    "        theta = theta_iter\n",
    "        \n",
    "        cost = regularized_logistic_cost_function([...]) # Calcular o custo para a atual iteração theta\n",
    "        \n",
    "        j_hist[...] # Adicionar o custo da iteração atual ao histórico de custos.\n",
    "        \n",
    "        # Verificar se a diferença entre o custo da iteração atual e o custo da última iteração em valor absoluto \n",
    "        é inferior à diferença mínima para declarar a convergência, e\n",
    "        # absoluto são inferiores que a diferença mínima para declarar a convergência, e\n",
    "        if k > 0 and [...]:\n",
    "            print('Convergir na iteração n.º: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('N.º máx. de iterações alcançado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formar um modelo de regressão logística não regularizado\n",
    "\n",
    "Para comprovar a implementação da sua função, utilize-o para formar um modelo de regressão logística no dataset sintéticos sem regularização (*lambda* = 0).\n",
    "\n",
    "Comprovar se o modelo converge corretamente para *Theta_verd*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprovar a sua implementação através da formação de um modelo no dataset sintético anteriormente criado.\n",
    "\n",
    "# Criar um theta inicial com um determinado valor.\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:') \n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1 \n",
    "lambda_ = 0. \n",
    "e = 1e-3 \n",
    "iter_ = 1e3\n",
    "\n",
    "print('Hiper-parâmetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_logistic_gradient_descent([...]) \n",
    "\n",
    "print('Tempo de formação (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores da função de custo') \n",
    "print(j_hist[...])\n",
    "print('\\Custo final:') \n",
    "print(j_hist[...]) \n",
    "print('\\nTheta final:') \n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdadeiros de Theta e diferença com valores formados:') \n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representar a evolução da função de custo \n",
    "\n",
    "Para comprovar a evolução da formação do seu modelo, representar graficamente o histórico da função custo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOD: Representar graficamente a função de custo vs. o número de iterações\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Função de custo') \n",
    "plt.xlabel('nº iterações') \n",
    "plt.ylabel('custo')\n",
    "\n",
    "plt.plot([...]) # Completar\n",
    "\n",
    "plt.grid() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
