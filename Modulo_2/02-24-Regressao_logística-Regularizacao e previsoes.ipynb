{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão logística: Regularização e previsões\n",
    "\n",
    "Uma vez implementada a função de regularização de custos e gradient descent, iremos formar um modelo completo de regressão logística, comprovando-o por validação cruzada, avaliando-o num subset de teste e, finalmente, fazendo previsões sobre o mesmo.\n",
    "\n",
    "Neste exercício, vamos continuar a implementar uma classificação entre 2 classes únicas. No exercício seguinte, vamos resolver o problema de uma classificação multiclasse.\n",
    "\n",
    "## O que vamos fazer?\n",
    "- Criar um dataset sintético para regressão logística. \n",
    "- Pré-processar os dados.\n",
    "- Formar o modelo sobre o subset de formação e comprovar a sua adequação. \n",
    "- Encontrar o parâmetro de regularização *lambda* ótimo por CV\n",
    "- Fazer previsões sobre novos exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar um dataset sintético para regressão logística\n",
    "\n",
    "Vamos criar um dataset sintético de 2 classes únicas (0 e 1) para testar esta implementação de um modelo de classificação binária, totalmente formado, passo a passo.\n",
    "\n",
    "Para isso, criar um dataset sintéticos para regressão logística com bias e termo de erro manualmente (para ter *Theta_verd* disponível) com um modelo de código que utilizou no último exercício:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Gerar um dataset sintéticos, com o termo de bias e erro de forma manual\n",
    "m = 1000\n",
    "n = 2\n",
    "\n",
    "Gerar um array 2D m x n com valores números aleatórios entre -1 e 1.\n",
    "# Inserir o termo de bias como primeira coluna de 1s\n",
    "X = [...]\n",
    "\n",
    "# Gerar um array de theta de n + 1 valores aleatórios\n",
    "Theta_verd = [...]\n",
    "\n",
    "# Calcular Y em função de X e Theta_verd\n",
    "Adicionar um termo de erro modificável\n",
    "# Transformar Y para valores de 1. e 0. (float) quando Y >= 0.5\n",
    "error = 0.20\n",
    "\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "Y = [...]\n",
    "\n",
    "# Verificar os valores e dimensões dos vetores\n",
    "print('Theta a estimar') \n",
    "print()\n",
    "\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:') \n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensões de X e Y:') \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota importante*: O termo de erro num dataset de classificação sintética funciona de forma diferente do que na regressão linear. Em regressão, modificaríamos simplesmente o valor de *Y* por uma margem.\n",
    "\n",
    "No entanto, na classificação, quando introduzimos esse erro, é antes de transformarmos esse valor numérico de *Y* para um valor de 0 ou 1. Portanto, se o termo de erro não fizer com que o valor numérico suba ou desça abaixo de 0,5, a classe associada a esse exemplo não irá mudar.\n",
    "\n",
    "Portanto, se achar que o seu dataset é “demasiado preciso” para poder comprovar a sua implementação da classificação regularizada, pode voltar a este ponto, aumentar o termo de erro e executar novamente o resto das células."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar a função de ativação sigmóide\n",
    "\n",
    "Copiar a sua célula com a função sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função sigmoide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processar os dados\n",
    "\n",
    "Tal como fizemos para a regressão linear, vamos pré-processar os dados completamente, seguindo os 3 passos habituais:\n",
    "\n",
    "- Reordená-los aleatoriamente. \n",
    "- Normalizá-los.\n",
    "- Dividi-los em subconjuntos de formação, CV e testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reordenar o dataset aleatoriamente\n",
    "\n",
    "Reorganizar os dados no dataset *X* e *Y*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reordenar aleatoriamente o dataset\n",
    "\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:') \n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Reordenamos X e Y:')\n",
    "# Se preferir, pode usar a função de conveniência sklearn.utils.shuffle\n",
    "# Utilizar um estado inicial aleatório de 42, de modo a manter a reprodutibilidade\n",
    "X, Y = [...]\n",
    "\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:') \n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensões de X e Y:') \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar o dataset\n",
    "\n",
    "Implementar a função de normalização e normalizar o dataset de exemplos *X*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalizar o dataset com uma função de normalização.\n",
    "\n",
    "def normalize(x, mu, std):\n",
    "    \"\"\" Normalizar um dataset com exemplos X\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    x -- array 2D de Numpy com os exemplos, sem termo de bias\n",
    "    mu -- vetor 1D de Numpy com a média de cada característica/coluna\n",
    "    std -- vetor 1D de Numpy com o desvio típico de cada característica/coluna\n",
    "    \n",
    "    Devolver:\n",
    "    X norm -- array 2D de Numpy com os exemplos, com as suas características normalizadas \n",
    "    \"\"\"\n",
    "    return [...]\n",
    "# Encontrar a média e o desvio padrão das características de X (colunas), exceto a primeira (parcialidade).\n",
    "mu = [...]\n",
    "std = [...]\n",
    "\n",
    "print('X original:') \n",
    "print(X) \n",
    "print(X.shape)\n",
    "\n",
    "print('Média e desvio típico das características:') \n",
    "print(mu)\n",
    "print(mu.shape) \n",
    "print(std) \n",
    "print(std.shape)\n",
    "\n",
    "print('X normalizada:') \n",
    "X_norm = np.copy(X)\n",
    "X_norm[...] = normalize(X[...], mu, std) # Normalizar apenas a coluna 1 e as colunas seguintes, não a coluna 0.\n",
    "print(X_norm) \n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: Se tiver modificado a sua função de *normalização* para calcular e devolver os valores de *mu* e *std*, pode modificar esta célula para incluir o seu código personalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir os dataset em subset de formação, CV e testes\n",
    "\n",
    "Dividir o dataset *X* e *Y* em 3 subsets com o ratio habitual, 60%/20%/20%.\n",
    "\n",
    "Se o seu número de exemplos for muito superior ou inferior, pode sempre modificar este ratio para outro, tal como 50/25/25 ou 80/10/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dividir o dataset X e Y nos 3 subsets, de acordo com os ratios indicados.\n",
    "\n",
    "\n",
    "ratios = [60,20,20]\n",
    "print('Ratios:\\n', ratios, ratios[0] + ratios[1] + ratios[2])\n",
    "\n",
    "r = [0,0]\n",
    "# Dica: a função round() e o atributo x.shape podem-lhe ser úteis\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Índices de corte:\\n', r)\n",
    "\n",
    "# Dica: a função np.array_split() pode ser útil para si\n",
    "X_train, X_cv, X_test = [...] \n",
    "Y_train, Y_cv, Y_test = [...]\n",
    "\n",
    "print('Tamanhos dos subsets:') \n",
    "print(X_train.shape) \n",
    "print(Y_train.shape) \n",
    "print(X_cv.shape) \n",
    "print(Y_cv.shape) \n",
    "print(X_test.shape) \n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formar um modelo inicial sobre o subset de formação.\n",
    "\n",
    "Tal como fizemos em exercícios anteriores, iremos formar um modelo inicial para verificar se a nossa implementação e o dataset funcionam corretamente, e seremos capazes de forma um modelo por CV sem qualquer problema.\n",
    "\n",
    "Para isso, seguir os mesmos passos que seguiu para a regressão linear:\n",
    "- Formar um modelo inicial sem regularização.\n",
    "- Representar o histórico do seu custo para comprovar a sua evolução.\n",
    "- Se necessário, modificar quaisquer parâmetros e reformular os modelos. Irá usar estes parâmetros nos pontos seguintes.\n",
    "\n",
    "Copiar as células dos exercícios anteriores onde implementou a função de regularização de custos e gradient descent para regressão logística, e a célula onde formou o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copiar células com funções de custo e gradient descent para classificação regularizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copiar a célula onde formamos o modelo\n",
    "# Formar o seu modelo no subset de formação sem regularizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar a evolução da função de custo vs. o número de iterações\n",
    "\n",
    "plt.figure(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprovar a adequação do modelo\n",
    "\n",
    "Verificar a precisão dos seus modelos e modificar os parâmetros para os formar de novo, se necessário.\n",
    "\n",
    "Recordar que se o seu dataset for “demasiado preciso” pode voltar à célula original e introduzir um termo de erro superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprovar se há desvio ou sobreajuste\n",
    "\n",
    "Como fizemos na regressão linear, formar 2 modelos em condições iguais, um sobre os primeiros dados do subset de formação e outro sobre o subset de CV (com o mesmo número de exemplos em ambos os casos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Estabelecer um theta_ini e hiper-parâmetros comuns para ambos os modelos, a fim de os formar em igualdade de\n",
    "# condiciçções\n",
    "\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:') \n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1 \n",
    "lambda_ = 0. \n",
    "e = 1e-3 \n",
    "iter_ = 1e3\n",
    "\n",
    "print('Hiper-parâmetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Formar um modelo sem regularização nos primeiros n valores do X_train, onde n é o número de\n",
    "# exemplos disponíveis em X_cv\n",
    "# Usar j_hist_train e theta_train como nomes de variáveis para os distinguir do outro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*:  Verificar se o *theta_ini* não foi modificado, ou modificar o seu código para que ambos os modelos utilizem o mesmo *theta_ini*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Da mesma forma, formar um modelo sem regularização em X_cv com os mesmos parâmetros.\n",
    "# Recordar de usar j_hist_cv e theta_cv como nomes de variável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora representar ambas as evoluções no mesmo gráfico, com cores diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar num gráfico de linhas ambas as evoluções para comparação.\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.title() \n",
    "plt.xlabel() \n",
    "plt.ylabel()\n",
    "\n",
    "# Usar cores diferentes para ambas as séries, e indicar uma legenda para os distinguir\n",
    "plt.plot() \n",
    "plt.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordar que com um dataset sintético aleatórios é difícil que um ou outro seja o caso, mas nesta forma poderíamos apreciar tais problemas da seguinte forma:\n",
    "\n",
    "- Se o custo final em ambos os subset for elevado, pode haver um problema de desvio ou *bias*.\n",
    "\n",
    "- Se o custo final em ambos os subset for muito diferente um do outro, pode haver um problema de sobreajuste ou *variação*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrar o hiper-parâmetro lambda ótimo por CV\n",
    "\n",
    "Como fizemos em exercícios anteriores, vamos otimizar o nosso parâmetro de regularização através de validação cruzada.\n",
    "\n",
    "Para isso, iremos formar um modelo diferente para cada valor *lambda* a ser considerado no subset de formação, e avaliar o seu erro ou custo final no subset de CV.\n",
    "\n",
    "Vamos representar graficamente o erro de cada modelo contra o valor *lambda* utilizado e implementar o código que irá escolher automaticamente o modelo ótimo.\n",
    "\n",
    "Recordar formar todos os seus modelos por igual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Formar um modelo para cada valor lambda diferente em X_train e avaliá-lo em X_cv\n",
    "\n",
    "lambdas = [0., 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1e0, 3e0, 1e1]\n",
    "\n",
    "# Completar o código para formar um modelo diferente para cada valor de lambda no X_train\n",
    "# Armazenar o theta e erro/custo final\n",
    "# Posteriormente, avaliar o seu custo total no subset da CV\n",
    "\n",
    "# Armazenar essa informação nos seguintes arrays, do mesmo tamanho que os lambdas\n",
    "j_train = [...]\n",
    "j_cv = [...]\n",
    "theta_cv = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar graficamente o erro final para cada valor de lambda\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "# Completar com o seu código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolher o melhor modelo\n",
    "\n",
    "Copiar o código dos exercícios anteriores, modificando-o se necessário para escolher o modelo com maior precisão no subset de CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escolher o modelo ótimo e o valor lambda, com o menor erro no subset do CV\n",
    "\n",
    "# Iterar sobre todas as combinações de theta e lambda e escolher o custo mais baixo no subset do CV\n",
    "\n",
    "j_final = [...] \n",
    "theta_final = [...] \n",
    "lambda_final = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar o modelo sobre o subset de teste\n",
    "\n",
    "Finalmente, vamos avaliar o modelo sobre um subset de dados que não utilizámos para formação ou para a escolha de quaisquer hiper-parâmetros.\n",
    "\n",
    "Para isso, vamos calcular o erro ou custo total no subset de teste e comprovar graficamente os resíduos no mesmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcular o erro do modelo no subset de teste usando a função de custo com a correspondente\n",
    "# theta e lambda\n",
    "\n",
    "j_test = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Calcular as previsões do modelo no subset de teste, calcular os resíduos e representá-los\n",
    "\n",
    "# Recordar de usar a função sigmoide para transformar as previsões.\n",
    "Y_test_pred = [...] \n",
    "\n",
    "resíduos = [...]\n",
    "\n",
    "plt.figure(4)\n",
    "\n",
    "# Completar com o seu código\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus*: Para além dos resíduos, *porque não representar também graficamente todas as previsões no subset de teste para comprovar quantos deles o nosso modelo acerta*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazer previsões sobre novos exemplos\n",
    "\n",
    "Com o nosso modelo formado, otimizado e avaliado, tudo o que resta é pô-lo a funcionar, fazendo previsões com novos exemplos.\n",
    "\n",
    "Para isso, vamos:\n",
    "- Gerar um novo exemplo seguindo o mesmo padrão que o dataset original. \n",
    "- Normalizar as suas características antes de poder fazer previsões a seu respeito. \n",
    "- Gerar uma previsão para esse novo exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Gerar um novo exemplo seguindo o padrão original, com termo de bias e erro aleatório.\n",
    "\n",
    "X_pred = [...]\n",
    "\n",
    "# Normalizar as suas características (exceto o termo de bias) com as médias e desvios típicos originais\n",
    "X_pred = [...]\n",
    "\n",
    "# Gerar uma previsão para esse exemplo\n",
    "Y_pred = [...]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
