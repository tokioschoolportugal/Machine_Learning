{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteção de anomalias: Dataset sintético\n",
    "\n",
    "## O que vamos fazer?\n",
    "\n",
    "- Criar um dataset sintético para a deteção de anomalias com casos normais e anómalos. \n",
    "- Modelar uma distribuição gaussiana sobre os dados normais.\n",
    "- Determinar o limiar de probabilidade de deteção de anómalos por validação. \n",
    "- Avaliar a precisão final do modelo sobre o subconjunto de teste.\n",
    "- Representar graficamente o comportamento do modelo em cada passo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Usar esta célula para importar todas as livrarias necessárias\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import from_levels_and_colors\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plot_n = 1\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar um dataset sintético para deteção de anomalias\n",
    "\n",
    "Para resolver este exercício, precisamos primeiro de criar um dataset com dados normais e outro com dados anómalos. Neste caso, os datasets serão 2 dimensões (2D) com apenas 2 características, em vez de um elevado n.º de características *n*, para facilitar a sua visualização numa representação 2D.\n",
    "\n",
    "Inicialmente, vamos criar 2 datasets independentes, um representando os dados normais e o outro os dados anómalos. Iremos combinar então estes dataset em 3 subsets finais, de formação, validação e teste, como é habitual, com a particularidade de, neste caso, os dados anómalos só serem encontrados nos subsets de validação e teste.\n",
    "\n",
    "Completar a seguinte célula de código para criar datasets iniciais independentes com dados normais e anómalos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Gerar dois datasets sintéticos independentes com dados normais e anómalos\n",
    "\n",
    "m = 300\n",
    "n = 2\n",
    "ratio_anomalos = 0.15 # Percentagem de dados anómalos vs dados normais, modificável\n",
    "m_anomalos = int(m * ratio_anomalos) \n",
    "m_normales = m - m_anomalos\n",
    "x_lim = (-5, 5)\n",
    "y_lim = (-5, 5)\n",
    "\n",
    "print('Nº de exemplos: {}, ratio de exemplos anómalos: {}%, nº de dados normais e anómalos {}/{}'.format(m, \n",
    "print('Nº de características: {}'.format(n))\n",
    "                                                                                                         \n",
    "# Criar ambos os dastasets\n",
    "dataset_normales = make_blobs(n_samples=m_normales, centers=np.array([[1.5, 1.5]]), cluster_std=1.0, random_st \n",
    "dataset_normales = dataset_normales[0] # Descartamos el resto de información y retenemos sólo las posicione \n",
    "dataset_anomalos = np.random.uniform(low=(x_lim[0], y_lim[0]), high=(x_lim[1], y_lim[1]), size=(m_anomalos, 2))\n",
    "\n",
    "# Representar os dados iniciais\n",
    "plt.figure(plot_n)\n",
    "                              \n",
    "plt.title('Dataset original: dados normais e anómalos') \n",
    "                              \n",
    "plt.scatter(dataset_normales[:, 0], dataset_normales[:, 1], s=10, color='b')\n",
    "plt.scatter(dataset_anomalos[:, 0], dataset_anomalos[:, 1], s=10, color='r')\n",
    "                              \n",
    "plt.xlim(x_lim) \n",
    "plt.ylim(y_lim)\n",
    "plt.legend(('Normais', 'Anómalos')) \n",
    "plt.grid()\n",
    "                              \n",
    "plt.show() \n",
    "                              \n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de continuar, vamos pré-processar os dados, normalizando-os, como habitualmente fazemos. Uma vez que o nosso *X* será composto por ambos os dataset, iremos normalizá-los à vez.\n",
    "\n",
    "Neste caso, não inserimos uma primeira coluna de 1s no dataset, pelo que normalizamos todas as colunas.\n",
    "\n",
    "Completar a célula de código seguinte para normalizar os dados. Para o fazer, resgatar a sua função de normalização dos exercícios anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalizar os dados de ambos datasets com os mesmos parâmetros de normalização\n",
    "\n",
    "def normalize(x, mu, std):\n",
    "    \"\"\" Normalizar um dataset com exemplos X\n",
    "    Argumentos posicionais:\n",
    "    x -- array 2D de Numpy com os exemplos, sem termo de bias\n",
    "    mu -- vetor 1D de Numpy com a média de cada característica/coluna\n",
    "    std -- vetor 1D de Numpy com o desvio típico de cada característica/coluna\n",
    "    Devolver:\n",
    "    X norm -- array 2D de Numpy com os exemplos, com as suas características \n",
    "    normalizadas \n",
    "    \"\"\"\n",
    "    return [...]\n",
    "\n",
    "# Encontrar a média e o desvio padrão das características dos datasets originais\n",
    "# Concatenar previamente ambos os datasets num X comum, certificando-se de utilizar o eixo correto.\n",
    "X = [...]\n",
    "mu_normalizar = [...] \n",
    "std = [...]\n",
    "\n",
    "print('Datasets originais:') \n",
    "print(dataset_normales.shape, dataset_anomalos.shape)\n",
    "\n",
    "print('Média e desvio típico das características:') \n",
    "print(mu_normalizar)\n",
    "print(mu_normalizar.shape) \n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Datasets normalizados:')\n",
    "dataset_normales_norm = normalize(dataset_normales, mu_normalizar, std) \n",
    "dataset_anomalos_norm = normalize(dataset_anomalos, mu_normalizar, std)\n",
    "\n",
    "print(dataset_normales_norm.shape) \n",
    "print(dataset_anomalos_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos subdividir os datasets originais nos subsets de formação, validação e teste.\n",
    "\n",
    "Para tal, dividimos o dataset normal de acordo com os ratios habituais, e atribuímos metade dos valores anómalos aos subsets de validação e teste. Se tivéssemos muito poucos dados anómalos, poderíamos incorporar a validação cruzada por K-fold.\n",
    "\n",
    "Completar a seguinte célula de código para criar estes subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dividir os datasets em subsets de formação, validação e teste com dados normais e anómalos\n",
    "\n",
    "ratios = [66,33,33]\n",
    "print('Ratios:\\n', ratios, ratios[0] + ratios[1] + ratios[2])\n",
    "\n",
    "r = [0,0]\n",
    "# Dica: a função round() e o atributo x.shape podem ser-lhe úteis\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Índices de corte:\\n', r)\n",
    "\n",
    "# Dividir o dataset de dados normais nos 3 subsets seguindo os ratios indicados\n",
    "# Dica: a função np.array_split() pode ser-lhe útil\n",
    "X_train, X_cv, X_test = [...]\n",
    "\n",
    "# Atribuir a etiqueta Y = 0 a todos os dados do dataset de dados normais.\n",
    "# Denotar os dados anómalos como Y = 1\n",
    "# Criar arrays 1D do comprimento do n.º de exemplos em cada subset com o valor 0. (float) em cada elemento\n",
    "Y_train = [...]\n",
    "Y_cv = [...]\n",
    "Y_test = [...]\n",
    "\n",
    "# Agora concatenar metade dos dados anómalos ao subset de validação e a outra metade ao subset de teste.\n",
    "# Dica:novamente a função np.array_split() pode ser útil para si\n",
    "dataset_anomalos_cv, dataset_anomalos_test = [...]\n",
    "\n",
    "X_cv = [...]\n",
    "X_test = [...]\n",
    "# O resultado final para X_cv e X_test serão vetores 2D de (m_normals * ratio[CV ou teste] + m_anomalos / 2,\n",
    "\n",
    "# Finalmente, como já fizemos antes, concatenar para Y_cv e Y_test cada uma das matrizes 1D com o comprimento \n",
    "de n\n",
    "# Cada matriz, desta vez, tem valores de 1. (float) em cada elemento\n",
    "Y_cv = [...]\n",
    "Y_test = [...]\n",
    "# O resultado final para Y_cv e Y_test será 1D vectores de (m_normals * ratio[CV ou teste], 1) de 0s e (m_ano\n",
    "\n",
    "# Comprovar os subsets criados\n",
    "print('Tamanhos dos subsets de formação, validação e teste:') \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape) \n",
    "print(X_cv.shape) \n",
    "print(Y_cv.shape) \n",
    "print(X_test.shape) \n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos acabar o pré-processamento dos datasets, reordenando-os aleatoriamente. \n",
    "\n",
    "Completar a seguinte célula de código para reordenar aleatoriamente os subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reordenar aleatoriamente los subconjuntos de entrenamiento, validación y prueba de forma individual\n",
    "\n",
    "print('Primeiras 10 filas e 2 colunas de X e vetor Y:') \n",
    "print('Subset de formação:')\n",
    "print(X_train[:10,:2])\n",
    "print(Y_train[:10,:2]) \n",
    "print('Subset de validação:') \n",
    "print(X_cv[:10,:2])\n",
    "print(Y_cv[:10,:2]) \n",
    "print('Subset de \n",
    "teste:') \n",
    "print(X_test[:10,:2])\n",
    "print(Y_test[:10,:2])\n",
    "      \n",
    "print('Reordenar X e Y:')\n",
    "# Se preferir, pode usar a função sklearn.utils.shuffle convenience function.\n",
    "# Utiliza um estado inicial aleatório de 42, de modo a manter a reprodutibilidade.\n",
    "X_train, Y_train = [...] \n",
    "X_cv, Y_cv = [...]\n",
    "X_test, Y_test = [...]\n",
    "      \n",
    "print('Primeiras 10 filas e 2 colunas de X e vetor Y:') \n",
    "print('Subset de formação:')\n",
    "print(X_train[:10,:2])\n",
    "print(Y_train[:10,:2]) \n",
    "print('Subset de validação:') \n",
    "print(X_cv[:10,:2])\n",
    "print(Y_cv[:10,:2]) \n",
    "print('Subset de \n",
    "teste:') \n",
    "print(X_test[:10,:2])\n",
    "print(Y_test[:10,:2])\n",
    "      \n",
    "print('Tamanhos dos subsets de formação, validação e teste:') \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape) \n",
    "print(X_cv.shape) \n",
    "print(Y_cv.shape) \n",
    "print(X_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, representar os nossos 3 subsets num gráfico 2D. \n",
    "\n",
    "Completar a seguinte célula de código para representar os subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar os 3 subsets num gráfico 2D\n",
    "\n",
    "# É possível ajustar parâmetros matplotlib, tais como a gama de dimensões e o tamanho dos pontos.\n",
    "plt.figure(plot_n)\n",
    "\n",
    "plt.title('Subsets com dados normais e anómalos')\n",
    "\n",
    "cmap, norm = from_levels_and_colors([0., 0.5, 1.1], ['blue', 'red'])\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=25, c=Y_train, marker='o', cmap=cmap, norm=norm) \n",
    "plt.scatter(X_cv[:, 0], X_cv[:, 1], s=25, c=Y_cv, marker='s', cmap=cmap, norm=norm) \n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=25, c=Y_test, marker='*', cmap=cmap, norm=norm)\n",
    "\n",
    "plt.xlim(x_lim) \n",
    "plt.ylim(y_lim)\n",
    "plt.legend(('formação', 'Validação', 'Teste')) \n",
    "plt.grid()\n",
    "\n",
    "plt.show() \n",
    "\n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelar uma distribuição gaussiana\n",
    "\n",
    "Vamos “formar o modelo”, o que neste caso irá significar modelar a presumível distribuição gaussiana que os dados normais seguem.\n",
    "\n",
    "Modelamos apenas os dados normais porque queremos encontrar que distribuição segue, que distribuição de dados é normal ou aceitável, e que dados não seguem tal distribuição e devem ser considerados anómalos.\n",
    "\n",
    "Uma distribuição gaussiana multivariável é definida por 2 parâmetros: a média $\\mu$ e a matriz de covariância $\\Sigma$. $\\mu$ é um vetor de tamanho\n",
    "(*n*) e $\\Sigma$ é um vetor/matriz quadrada (*n*, *n*).\n",
    "\n",
    "Recordar do módulo e exercício sobre SVM com filtro gaussiano que a distribuição multivariada gaussiana (ou normal) pode ter uma forma arredondada ou oval, que o $\\mu$ representa o ponto central da distribuição no espaço e o $\\Sigma$ a forma da mesma.\n",
    "\n",
    "*NOTA*: Embora a distribuição normal ou gaussiana seja uma das, se não a mais comum na natureza, num projeto real devemos primeiro verificar se os nossos dados normais, de acordo com o conjunto de características retiradas, seguem uma distribuição normal ou temos de os modelar com outro tipo de distribuição, seguindo os mesmos passos. \n",
    "\n",
    "μ e Σ poderiam ser calculados como:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum\\limits_{i=0}^{m} x^i; \\\\\n",
    "\\Sigma = \\frac{1}{m} \\sum\\limits_{i=0}^{m} (x^i - \\mu)(x^i - \\mu)^T;\n",
    "$$\n",
    "\n",
    "Seguir as instruções abaixo para modelar a distribuição gaussiana e obter os seus parâmetros $\\mu$ e $\\Sigma$ e depois calcular a probabilidade de um ponto ser anómalo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modelar a distribuição gaussiana e obter mu e Sigma\n",
    "\n",
    "# Calcular a média e Sigma de X_train\n",
    "# Para o fazer, pode usar as funções de matriz de média e covariância do Numpy com o eixo apropriado.\n",
    "mu = [...]\n",
    "sigma = [...]\n",
    "\n",
    "# Computar a distribuição normal multivariável com estes parâmetros\n",
    "dist_normal = multivariado_normal(média=mu, cov=sigma)\n",
    "\n",
    "print('Dimensões da média e matriz de covariância do subset de formação:')\n",
    "print(mu.shape, sigma.shape)\n",
    "print('Média:') \n",
    "print(mu)\n",
    "print('Matriz de \n",
    "covariância:') \n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos representar a função de densidade da distribuição normal de dados com fatias de probabilidade ao lado do dataset normal.\n",
    "\n",
    "Função de densidade de probabilidade:\n",
    "\n",
    "$pdf(x) = \\frac{1}{\\Sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2}(\\frac{x - \\mu}{\\Sigma})^2}$\n",
    "\n",
    "Seguir as instruções na célula seguinte para o fazer\n",
    "\n",
    "*NOTA*: Pode basear-se na função [contourf](https://matplotlib.org/stable/gallery/images_contours_and_fields/contourf_demo.html#sphx-glr-gallery-images-contours-and-fields-contourf-demo-py) de matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: representar a função de densidade e os dados normais\n",
    "\n",
    "fig1, ax2 = plt.subplots([...])\n",
    "\n",
    "# Adicionar uma barra de cor com a probabilidade da distribuição\n",
    "[...]\n",
    "\n",
    "# Adicionar um título e etiquetas a cada dimensão\n",
    "[...]\n",
    "\n",
    "# Também representar os dados do subset de formação X_train como pontos no mesmo gráfico.\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinar el umbral de probabilidad para detectar casos anómalos\n",
    "\n",
    "Vamos agora determinar o limiar de probabilidade em que determinaremos se um novo caso é normal ou anómalo. Se um exemplo for demasiado diferente dos dados normais, se estiver longe dos dados normais, se a probabilidade de seguir a mesma distribuição que os dados normais, for inferior a este limiar, podemos declarar como anómalo.\n",
    "\n",
    "Para encontrar este limiar, utilizaremos o subset de validação, com dados normais e anómalos, e tal como a validação para regularização na aprendizagem supervisionada, iremos estimars múltiplos valores do limiar  $\\epsilon$, mantendo aquele que melhor classifica os dados normais e anómalos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para começar, vamos representar graficamente a distribuição, o subset de validação e vários valores possíveis de $\\epsilon$. \n",
    "\n",
    "Para isso, seguir as instruções para completar a seguinte célula\n",
    "\n",
    "*NOTA*: Para linhas de contorno, utilizar a função de [contour](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.contour.html#matplotlib.axes.Axes.contour), também utilizada no exemplo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar a distribuição, o subset de validação e vários valores possíveis do epsilon\n",
    "\n",
    "# Gera alguns valores para epsilon\n",
    "epsilon_evaluated = np.linspace(0., 0.5, num=5)\n",
    "\n",
    "# Recuperar o código da célula anterior e adicionar uma linha de contorno a cada valor de epsilon\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ter mais visibilidade no nosso dataset e para comprovar o valor de $\\epsilon$ finalmente, vamos calcular as probabilidades de cada dado anómalo no subset de validação, e seguir a distribuição de dados normais.\n",
    "\n",
    "Para isso, seguir as instruções na célula seguinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcular as probabilidades dos dados anómalos de validação de acordo com a distribuição da formação\n",
    "\n",
    "# Filtrar os dados no subset de validação que são anómalos\n",
    "# Recordar que os dados anómalos têm um Y_cv = 1.\n",
    "X_cv_anomalos = [...]\n",
    "\n",
    "# Calcular as suas probabilidades de seguir a distribuição normal\n",
    "p_cv_anomalos = dist_normal.pdf(X_cv_anomalos)\n",
    "\n",
    "print('Probabilidade de seguir a distribuição normal dos 10 primeiros dados de validação:')\n",
    "print(p_cv_anomalos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos avaliar um espaço linear de valores possíveis de $\\epsilon$ e encontrar o mais ideal para declarar um dado como anómalo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Avaliar múltiplos valores de epsilon e encontrar o melhor para classificar os dados como normais ou anómalos.\n",
    "\n",
    "# Gerar um espaço linear de valores epsilon com mais precisão\n",
    "epsilon_evaluated = np.linspace(0., 1., num=1e2) # Pode modificar a precisão para acelerar o cálculo\n",
    "\n",
    "# Valores para encontrar o seu ótimo\n",
    "epsilon = 1e6 # Valor do epsilon\n",
    "f1_cv = 0. # F1_score da classificação\n",
    "for e in epsilon_evaluado:\n",
    "    # Atribuir Y = 1. a valores cuja probabilidade é inferior a epsilon e 0. ao resto.\n",
    "    Y_cv_pred = np.where([...])\n",
    "    \n",
    "    # Encontrar a pontuação F1 para essa classificação com Y_cv como o valor conhecido.\n",
    "    score = f1_score([...])\n",
    "    \n",
    "    if score > f1_cv: \n",
    "        f1_cv = score \n",
    "        epsilon = e\n",
    "        \n",
    "print('Epsilon ótima no subset de validação:', epsilon) \n",
    "print('Com F1-score:', f1_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar a precisão final do modelo\n",
    "\n",
    "Para concluir a nossa formação, vamos verificar a precisão final do modelo no subset de teste, como habitualmente fazemos.\n",
    "\n",
    "Para tal, iremos fazer uma verificação matemática e visual dos dados.\n",
    "\n",
    "Seguir as instruções para preencher a célula seguinte e traçar os valores normais e anómalas do subset de teste juntamente com a distribuição normal dos dados do subset de formação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar o subset de teste juntamente com a distribuição de dados do subset de formação.\n",
    "# Incluir a linha de contorno para o epsilon escolhido.\n",
    "\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora calcular a métrica de avaliação da classificação para avaliar a classificação entre dados normais e anómalos feita pelo modelo no subset de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcular a métrica de avaliação da classificação do modelo para o subset de teste.\n",
    "\n",
    "# Atribuir Y = 1. a valores cuja probabilidade é inferior a epsilon e 0. ao resto.\n",
    "Y_test_pred = np.where([...])\n",
    "\n",
    "# Encontrar a pontuação F1 para essa classificação com Y_teste como o valor conhecido.\n",
    "f1_test = f1_score([...])\n",
    "\n",
    "print('F1-score para o subset de teste:', f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisar graficamente quais os dados do subset de teste que o modelo classifica correta e incorretamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar erros e acertos no subset de teste junto à distribuição e o corte de epsilon\n",
    "\n",
    "# Atribuir z = 1. para acerto e z = 0. para falha\n",
    "# Acerto: Y_test == Y_test_pred\n",
    "z = [...]\n",
    "\n",
    "# Representar o gráfico\n",
    "# Utilizar cores diferentes para os dados que acertou e os que falhou\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Acha que o modelo faz um bom trabalho na deteção de anomalias?*\n",
    "\n",
    "*Existe algum dado que classificaria de forma diferente?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, representar graficamente todos os dados, dos 3 subsets, juntamente com a distribuição e o corte $\\epsilon$, para analisar a distribuição de dados normais e anómalos e o funcionamento do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representar os dados normais e anómalos juntamente com a distribuição e o corte do epsilon.\n",
    "# Representar os 3 subsets: formação, validação e teste\n",
    "# Se preferir, pode distingui-los com marcadores de diferentes formas\n",
    "# Pode usar cores diferentes para dados normais e anómalos que eram originalmente conhecidos\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
