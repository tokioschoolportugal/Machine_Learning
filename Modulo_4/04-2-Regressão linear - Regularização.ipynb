{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear: Regularização\n",
    "\n",
    "## O que vamos fazer?\n",
    "- Implementar a função de custo regularizada para a regressão linear multivariável \n",
    "- Implementar a regularização para o gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de um dataset sintético\n",
    "\n",
    "Para comprovar a sua implementação de uma função de custo e gradient descent regularizado, resgatar as suas células dos notebooks anteriores para os dataset sintéticos e gerar um dataset para este exercício.\n",
    "\n",
    "Não esquecer de acrescentar um termo de bias a *X* e um termo de erro a *Y*, inicializado a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Gerar um dataset sintéticos manualmente, com o termo de bias e o termo de erro inicializados a 0.\n",
    "\n",
    "m = 1000\n",
    "n = 3\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_verd = [...]\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Comprovar os valores e dimensões dos vetores\n",
    "print('Theta a estimar e as suas dimensões') \n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Primeiras 10 filas e 5 colunas de X e Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensões de X e Y:') \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de custo regularizada\n",
    "\n",
    "Agora vamos modificar a nossa implementação da função de custo para adicionar o termo de regularização. \n",
    "\n",
    "Recordar que a função de custo regularizada é:\n",
    "\n",
    "$J_\\theta = \\frac{1}{2m} [\\sum\\limits_{i=0}^{m} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função de custo regularizada utilizando o seguinte modelo\n",
    "\n",
    "def regularized_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computar a função de custo para o dataset e coeficientes considerados.\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "     x -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, de tamanho m x 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila)\n",
    "    \n",
    "    Argumentos numerados:\n",
    "    lambda -- float com o parâmetro de regularização\n",
    "    \n",
    "    Devolver:\n",
    "     j -- float com o custo regularizado para esse array theta \n",
    "     \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Recordar de verificar as dimensões da multiplicação da matriz para fazer corretamente.\n",
    "    # Recordar de não regularizar o coeficiente do parâmetro bias (primeiro valor do theta).\n",
    "    j = [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o dataset sintético tem o termo de erro a 0, a função de custo para o Theta_verd com o parâmetro lambda 0 deve ser exatamente 0.\n",
    "\n",
    "Como antes, à medida que nos afastamos com valores diferentes de theta, o custo deve aumentar. Do mesmo modo, quanto maior for o parâmetro de regularização, maior será a penalização e o custo, e quanto maior for o valor do theta, maior será a penalização e o custo.\n",
    "\n",
    "Comprovar a sua implementação nestas 5 circunstâncias:\n",
    "- Usando Theta_verd e com lambda a 0, o custo deve continuar a ser 0.\n",
    "- Com lambda 0 ainda, à medida que os valores theta se afastam de Theta_verd, o custo deve ser maior. \n",
    "- Usando Theta_verd e com lambda diferente de 0, o custo deve agora ser superior a 0.\n",
    "- Com lambda diferente de 0, para um theta diferente de Theta_verd o custo deve ser maior do que com lambda igual a 0.\n",
    "- Com lambda diferente de 0, quanto mais elevados forem os valores dos coeficientes de theta (positivos ou negativos*), maior será a penalização e maior será o custo.\n",
    "\n",
    "Recordar que o valor do lambda deve ser sempre positivo e inferior a 0: [0, 1e-1, 3e-1, 1e-2, 3e-2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprovar a implementação da sua função de custos regularizada nessas circunstâncias\n",
    "\n",
    "theta = Theta_verd # Modificar e testar vários valores do theta\n",
    "\n",
    "j = regularized_cost_function(X, Y, theta) \n",
    "\n",
    "print('Custo do modelo:')\n",
    "print(j)\n",
    "print('Theta comprovado e Theta real:') \n",
    "print(theta)\n",
    "print(Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anotar os seus resultados nesta célula:\n",
    "\n",
    "1. Resultado1\n",
    "1. Resultado2\n",
    "1. Resultado3\n",
    "1. Resultado4\n",
    "1. Resultado5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent regularizado\n",
    "\n",
    "Agora vamos regularizar também a formação por gradient descent. Vamos modificar as atualizações de *Theta* para que agora contenham também o parâmetro de regularização *lambda*:\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i \\\\\n",
    "\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i + \\frac{\\lambda}{m} \\theta_j] \\\\\n",
    "\\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i \\\\\n",
    "j \\in [1, n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar a função que forma o modelo por gradient descent regularizado\n",
    "\n",
    "def regularized_gradient_descent(x, y, theta, alpha, lambda_=0., e, iter_):\n",
    "    \"\"\" Formar o modelo otimizando a sua função de custo por gradient descent\n",
    "    \n",
    "    Argumentos posicionais:\n",
    "    x -- array 2D de Numpy com os valores das variáveis independentes dos exemplos, de tamanho m x n \n",
    "    y -- array 1D Numpy com a variável dependente/objetivo, de tamanho m x 1\n",
    "    theta -- array 1D Numpy com os pesos dos coeficientes do modelo, de tamanho 1 x n (vetor fila) \n",
    "    alpha -- float, ratio de formação\n",
    "    \n",
    "    Argumentos numerados (keyword):\n",
    "    lambda -- float com o parâmetro de regularização\n",
    "    e -- float, diferença mínima entre iterações para declarar que a formação finalmente convergiu \n",
    "    iter_ -- int/float, número de iterações\n",
    "    \n",
    "    Devolver:\n",
    "    j_hist -- list/array com a evolução da função de custo durante a formação \n",
    "    theta -- array Numpy com o valor do theta na última iteração\n",
    "    \"\"\"\n",
    "    # TODO: declarar valores por defeito para e e iter_ nos argumentos nomeados (palavra-chave) da função.\n",
    "    \n",
    "    iter_ = int(iter_) # Se declarou iter_ em notação científica (1e3) ou float (1000.), converter\n",
    "    \n",
    "    # Inicializar j_hist como uma list ou um array Numpy. Recordar que não sabemos que tamanho terá eventualmente\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...] # Obter m e n a partir das dimensões de X\n",
    "   \n",
    "    for k in [...]: # Iterar sobre o número máximo de iterações\n",
    "        theta_iter = [...] # Declarar um theta para cada iteração, pois precisamos de a atualizar.\n",
    "        \n",
    "        for j in [...]: # Iterar sobre o número de características\n",
    "            # Atualizar theta_iter para cada característica, de acordo com a derivada da função de custo\n",
    "            # Incluir o ratio de formação alpha\n",
    "            # Cuidado com as multiplicações matriciais, a sua ordem e dimensões\n",
    "            \n",
    "            if j > 0:\n",
    "                pass # Regularizar tudo coeficiente exceto o do parâmetro bias (primeiro coef.)\n",
    "            \n",
    "            theta_iter[j] = theta[j] - [...] \n",
    "            \n",
    "        theta = theta_iter\n",
    "        \n",
    "        cost = cost_function([...]) # Calcular o custo para a atual iteração theta\n",
    "        \n",
    "        j_hist[...] # Adicionar o custo da iteração atual ao histórico de custos.\n",
    "        \n",
    "        # Comprovar se a diferença entre o custo da iteração atual e o custo da última iteração em valor absoluto é  inferior à diferença mínima para declarar a convergência, e\n",
    "        # absoluto são inferiores que a diferença mínima para declarar a convergência, e\n",
    "        if k > 0 and [...]:\n",
    "            print('Convergir na iteração n.º: ', k)\n",
    "            break\n",
    "    else:\n",
    "        print('N.º máx. de iterações alcançado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: Recordar que os modelos de código são apenas uma ajuda. Ocasionalmente, poderá querer usar códigos diferentes com a mesma funcionalidade, por exemplo, iterar sobre elementos de uma forma diferente, etc. Sinta-se à vontade para os modificar como desejar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprovar a sua implementação, mais uma vez, comprovar com *lambda* usando vários valores de *Theta*, tanto o *Theta_verd* como valores cada vez mais afastados do mesmo, e comprovar se eventualmente o modelo converge para o *Theta_verd* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Testar a sua implementação através da formação de um modelo no dataset sintético anteriormente criado.\n",
    "\n",
    "# Criar um theta inicial com um determinado valor.\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:') \n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1 \n",
    "lambda_ = 0. \n",
    "e = 1e-3\n",
    "iter_ = 1e3 # Verificar se a sua função pode suportar valores de float ou modificá-los.\n",
    "\n",
    "print('Hiper-parâmetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_gradient_descent([...]) \n",
    "\n",
    "print('Tempo de formação (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores da função de custo') \n",
    "print(j_hist[...])\n",
    "print('\\Custo final:') \n",
    "print(j_hist[...]) \n",
    "print('\\nTheta final:') \n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdadeiros de Theta e diferença com valores formados:') \n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora confirmar novamente a formação de um modelo em algumas das circunstâncias acima referidas:\n",
    "\n",
    "- Usando um theta_ini aleatório e com lambda a 0, o custo final deverá ser ainda próximo de 0 e o theta final próximo de Theta_verd. \n",
    "- Usando um theta_ini aleatório e com lambda pequeno e diferente de 0, o custo final deve ser próximo de 0, embora o modelo possa começar a perder precisão.\n",
    "- À medida que o valor lambda aumenta, o modelo vai perdendo mais precisão.\n",
    "\n",
    "Lembre-se que pode alterar os valores das células e voltar a executar as células. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anotar os seus resultados nesta célula:\n",
    "\n",
    "1. Resultado1\n",
    "1. Resultado2\n",
    "1. Resultado3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
